# üöÄ Gu√≠a de Implementaci√≥n: Unified AI API Gateway

**Autor:** IAFasioBoy (Asistente IA de Felix Rodr√≠guez Roble)  
**Fecha:** 31 de enero de 2026  
**Email:** felix.roble@tecnotactil.com

---

## üìã Tabla de Contenido

1. [Visi√≥n General](#visi√≥n-general)
2. [Arquitectura](#arquitectura)
3. [Requisitos del Servidor](#requisitos-del-servidor)
4. [Instalaci√≥n de Modelos Locales](#instalaci√≥n-de-modelos-locales)
5. [Implementaci√≥n del Proxy API](#implementaci√≥n-del-proxy-api)
6. [Configuraci√≥n](#configuraci√≥n)
7. [Deployment](#deployment)
8. [Uso](#uso)
9. [Mantenimiento](#mantenimiento)

---

## 1. üéØ Visi√≥n General

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Unified AI API Gateway                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ   Cliente (cualquier app)                                       ‚îÇ
‚îÇ        ‚îÇ                                                        ‚îÇ
‚îÇ        ‚ñº                                                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ   ‚îÇ  Flask API (puerto 8080)                   ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ  - Compatible OpenAIÊ†ºÂºè                    ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ  - Rate limiting                           ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ  - Load balancing                          ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ  - Fallback autom√°tico                     ‚îÇ              ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ              ‚îÇ                           ‚îÇ                     ‚îÇ
‚îÇ              ‚ñº                           ‚ñº                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ   ‚îÇ Modelos Locales  ‚îÇ        ‚îÇ  DeepSeek API    ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ (Ollama/Llama)   ‚îÇ        ‚îÇ  (Fallback)      ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ Qwen 7B          ‚îÇ        ‚îÇ  chat/completion ‚îÇ            ‚îÇ
‚îÇ   ‚îÇ Llama 3B         ‚îÇ        ‚îÇ                  ‚îÇ            ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Caracter√≠sticas

- ‚úÖ **Compatible OpenAI API** - Tus apps funcionan igual
- ‚úÖ **Modelos locales gratuitos** - Qwen, Llama, Mistral
- ‚úÖ **Fallback autom√°tico** - Si el local falla, usa DeepSeek
- ‚úÖ **Rate limiting** - Control de uso
- ‚úÖ **Logging** - Control de costos

---

## 2. üèóÔ∏è Arquitectura

```
unified-ai-api/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py          # Endpoint /v1/chat/completions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py        # Endpoint /v1/models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py        # Health checks
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local_model.py   # Ollama integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ remote_model.py  # DeepSeek API
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ router.py        # Decision engine
‚îÇ   ‚îî‚îÄ‚îÄ middleware/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ auth.py          # API key validation
‚îÇ       ‚îú‚îÄ‚îÄ rate_limit.py    # Rate limiting
‚îÇ       ‚îî‚îÄ‚îÄ logging.py       # Request/response logging
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py          # Configuraci√≥n principal
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py           # System prompts
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ run.py
‚îî‚îÄ‚îÄ README.md
```

---

## 3. üíª Requisitos del Servidor

### Servidor Dedicado (recomendado)

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| CPU | 8 n√∫cleos | 12+ n√∫cleos |
| RAM | 16 GB | 24 GB |
| Storage | 100 GB SSD | 500 GB SSD |
| GPU | Opcional | NVIDIA 24GB VRAM |
| Red | 100 Mbps | 1 Gbps |

### Modelos Recomendados seg√∫n RAM

| RAM Disponible | Modelo Local | Tama√±o |
|----------------|--------------|--------|
| 8 GB | Qwen2.5 3B | ~4GB |
| 16 GB | Qwen2.5 7B | ~7GB |
| 24 GB | Qwen2.5 14B | ~14GB |
| 32 GB+ | Qwen2.5 32B | ~32GB |

---

## 4. üê≥ Instalaci√≥n de Modelos Locales (Ollama)

### Paso 1: Instalar Ollama

```bash
# En el servidor dedicado
curl -fsSL https://ollama.ai/install.sh | sh

# Iniciar servicio
sudo systemctl start ollama
sudo systemctl enable ollama

# Verificar instalaci√≥n
ollama --version
```

### Paso 2: Descargar Modelos

```bash
# Modelo recomendado para 24GB RAM
ollama pull qwen2.5:7b          # ~7GB - Conversaci√≥n general

# Modelos adicionales
ollama pull llama3.2:3b         # ~4GB - Chat r√°pido
ollama pull deepseek-r1:7b      # ~7B - Reasoning
ollama pull mistral:7b          # ~7GB - Balanceado

# Ver modelos instalados
ollama list
```

### Paso 3: Configurar Ollama como API

```bash
# Crear servicio systemd
sudo nano /etc/systemd/system/ollama.service

# Contenido:
[Unit]
Description=Ollama AI Model Server
After=network.target

[Service]
Type=simple
User=ollama
WorkingDirectory=/home/ollama
Environment="OLLAMA_HOST=0.0.0.0:11434"
ExecStart=/usr/local/bin/ollama serve
Restart=always

[Install]
WantedBy=multi-user.target

# Activar
sudo systemctl daemon-reload
sudo systemctl start ollama
sudo systemctl enable ollama
```

---

## 5. üîß Implementaci√≥n del Proxy API

### requirements.txt

```txt
flask>=2.3.0
flask-cors>=4.0.0
gunicorn>=21.0.0
gevent>=23.0.0
requests>=2.31.0
python-dotenv>=1.0.0
redis>=5.0.0
apscheduler>=3.10.0
```

### Archivo Principal: run.py

```python
#!/usr/bin/env python3
"""
Unified AI API Gateway
Compatible con OpenAI API - Puede usar modelos locales o remotos
"""

from flask import Flask, jsonify, request
from flask_cors import CORS
from api.routes.chat import chat_bp
from api.routes.models import models_bp
from api.routes.health import health_bp
from config.settings import load_config

app = Flask(__name__)
CORS(app)

# Cargar configuraci√≥n
config = load_config()

# Registrar blueprints
app.register_blueprint(health_bp, url_prefix='/health')
app.register_blueprint(models_bp, url_prefix='/v1')
app.register_blueprint(chat_bp, url_prefix='/v1/chat')

@app.errorhandler(404)
def not_found(e):
    return jsonify({"error": "Not found", "message": str(e)}), 404

@app.errorhandler(500)
def server_error(e):
    return jsonify({"error": "Internal server error", "message": str(e)}), 500

if __name__ == '__main__':
    host = config.get('HOST', '0.0.0.0')
    port = config.get('PORT', 8080)
    debug = config.get('DEBUG', False)
    
    print(f"üöÄ Unified AI API Gateway iniciado en http://{host}:{port}")
    app.run(host=host, port=port, debug=debug)
```

### Endpoint de Chat: api/routes/chat.py

```python
#!/usr/bin/env python3
"""
API Routes - Chat Completions
Compatible con OpenAI /v1/chat/completions
"""

from flask import Blueprint, request, jsonify
from api.services.router import AIRouter
from api.middleware.auth import require_api_key
from api.middleware.rate_limit import check_rate_limit
import logging

chat_bp = Blueprint('chat', __name__)
router = AIRouter()

@chat_bp.route('/completions', methods=['POST'])
@require_api_key
@check_rate_limit
def create_completion():
    """
    Endpoint compatible con OpenAI Chat Completions
    
    POST /v1/chat/completions
    {
        "model": "qwen2.5:7b",
        "messages": [
            {"role": "system", "content": "Eres un asistente √∫til."},
            {"role": "user", "content": "Hola, ¬øc√≥mo est√°s?"}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    }
    """
    try:
        data = request.json
        
        # Validar par√°metros requeridos
        if not data:
            return jsonify({"error": "No input data provided"}), 400
        
        if 'messages' not in data:
            return jsonify({"error": "Missing 'messages' parameter"}), 400
        
        if 'model' not in data:
            return jsonify({"error": "Missing 'model' parameter"}), 400
        
        # Extraer par√°metros
        model = data.get('model')
        messages = data.get('messages')
        temperature = data.get('temperature', 0.7)
        max_tokens = data.get('max_tokens', 2000)
        stream = data.get('stream', False)
        
        # Routing: local o remoto
        response = router.route_request(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=stream
        )
        
        return jsonify(response)
        
    except Exception as e:
        logging.error(f"Error en completions: {e}")
        return jsonify({
            "error": "Internal server error",
            "message": str(e)
        }), 500
```

### Router Inteligente: api/services/router.py

```python
#!/usr/bin/env python3
"""
AI Router - Decide entre modelo local o remoto
"""

import requests
import json
import logging
from config.settings import load_config

class AIRouter:
    """
    Decide qu√© modelo usar basado en:
    - Complejidad de la tarea
    - Disponibilidad de modelos locales
    - Configuraci√≥n de fallback
    """
    
    def __init__(self):
        self.config = load_config()
        self.ollama_url = self.config.get('OLLAMA_URL', 'http://localhost:11434')
        self.deepseek_url = self.config.get('DEEPSEEK_URL', 'https://api.deepseek.com/chat/completions')
        self.deepseek_key = self.config.get('DEEPSEEK_API_KEY', '')
        
        # Modelos que van directamente a DeepSeek
        self.remote_models = ['deepseek', 'reasoner', 'coder']
        
        # Umbral de complejidad (chars)
        self.complexity_threshold = 500
    
    def is_complex_request(self, messages):
        """Determina si la solicitud es compleja"""
        total_chars = sum(len(str(m.get('content', ''))) for m in messages)
        return total_chars > self.complexity_threshold
    
    def is_local_model_available(self, model):
        """Verifica si el modelo local est√° disponible"""
        try:
            response = requests.get(
                f"{self.ollama_url}/api/tags",
                timeout=5
            )
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                return any(model in name for name in model_names)
        except:
            pass
        return False
    
    def route_request(self, model, messages, temperature, max_tokens, stream):
        """
        Rutarea la solicitud al modelo apropiado
        """
        # 1. Si el modelo es expl√≠citamente remoto
        if any(rm in model.lower() for rm in self.remote_models):
            return self.call_deepseek(model, messages, temperature, max_tokens, stream)
        
        # 2. Si es una solicitud compleja ‚Üí DeepSeek
        if self.is_complex_request(messages):
            logging.info(f"Solicitud compleja detectada ‚Üí DeepSeek")
            return self.call_deepseek(model, messages, temperature, max_tokens, stream)
        
        # 3. Si el modelo local est√° disponible ‚Üí usar local
        if self.is_local_model_available(model):
            logging.info(f"Modelo local disponible ‚Üí Ollama ({model})")
            return self.call_ollama(model, messages, temperature, max_tokens, stream)
        
        # 4. Fallback a DeepSeek
        logging.info(f"Sin modelo local disponible ‚Üí Fallback a DeepSeek")
        return self.call_deepseek(model, messages, temperature, max_tokens, stream)
    
    def call_ollama(self, model, messages, temperature, max_tokens, stream):
        """Llama al modelo local v√≠a Ollama"""
        # Convertir mensajes de formato OpenAI a Ollama
        ollama_messages = self._convert_to_ollama(messages)
        
        payload = {
            "model": model,
            "messages": ollama_messages,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            },
            "stream": stream
        }
        
        response = requests.post(
            f"{self.ollama_url}/api/chat",
            json=payload,
            timeout=120
        )
        
        if response.status_code == 200:
            return self._convert_from_ollama(response.json(), model)
        else:
            # Si falla Ollama, fallback a DeepSeek
            logging.warning(f"Ollama fall√≥ ({response.status_code}) ‚Üí Fallback a DeepSeek")
            return self.call_deepseek(model, messages, temperature, max_tokens, stream)
    
    def call_deepseek(self, model, messages, temperature, max_tokens, stream):
        """Llama a DeepSeek API"""
        payload = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream
        }
        
        headers = {
            "Authorization": f"Bearer {self.deepseek_key}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(
            self.deepseek_url,
            json=payload,
            headers=headers,
            timeout=120
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"DeepSeek API error: {response.status_code}")
    
    def _convert_to_ollama(self, messages):
        """Convierte mensajes de OpenAI a formato Ollama"""
        ollama_messages = []
        system_prompt = None
        
        for msg in messages:
            if msg.get('role') == 'system':
                system_prompt = msg.get('content', '')
            else:
                ollama_messages.append({
                    "role": msg.get('role', 'user'),
                    "content": msg.get('content', '')
                })
        
        # Agregar system prompt al primer mensaje si existe
        if system_prompt and ollama_messages:
            ollama_messages[0]['content'] = f"{system_prompt}\n\n{ollama_messages[0]['content']}"
        
        return ollama_messages
    
    def _convert_from_ollama(self, response, original_model):
        """Convierte respuesta de Ollama a formato OpenAI"""
        # Formato simplificado - ajustar seg√∫n necesidad
        return {
            "id": f"chatcmpl-{response.get('id', 'local')}",
            "object": "chat.completion",
            "created": 1234567890,
            "model": original_model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response.get('message', {}).get('content', '')
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
```

### Middleware de Auth: api/middleware/auth.py

```python
#!/usr/bin/env python3
"""
API Key Authentication
"""

from functools import wraps
from flask import request, jsonify
import os

def get_api_key():
    """Obtiene API key del header o query param"""
    # Header Authorization: Bearer sk-...
    auth_header = request.headers.get('Authorization', '')
    if auth_header.startswith('Bearer '):
        return auth_header[7:]
    
    # Query param ?api_key=...
    return request.args.get('api_key', '')

def require_api_key(f):
    """Decorator para requerir API key"""
    @wraps(f)
    def decorated(*args, **kwargs):
        valid_keys = os.environ.get('API_KEYS', '').split(',')
        api_key = get_api_key()
        
        if not api_key or api_key not in valid_keys:
            return jsonify({
                "error": "Invalid or missing API key",
                "message": "Please provide a valid API key"
            }), 401
        
        return f(*args, **kwargs)
    return decorated
```

### Configuraci√≥n: config/settings.py

```python
#!/usr/bin/env python3
"""
Configuraci√≥n del sistema
"""

import os
from dotenv import load_dotenv

load_dotenv()

def load_config():
    """Carga configuraci√≥n desde variables de entorno"""
    return {
        'HOST': os.getenv('HOST', '0.0.0.0'),
        'PORT': int(os.getenv('PORT', 8080)),
        'DEBUG': os.getenv('DEBUG', 'False').lower() == 'true',
        
        # Ollama
        'OLLAMA_URL': os.getenv('OLLAMA_URL', 'http://localhost:11434'),
        
        # DeepSeek
        'DEEPSEEK_URL': os.getenv('DEEPSEEK_URL', 'https://api.deepseek.com/chat/completions'),
        'DEEPSEEK_API_KEY': os.getenv('DEEPSEEK_API_KEY', ''),
        
        # Rate Limiting
        'RATE_LIMIT_REQUESTS': int(os.getenv('RATE_LIMIT_REQUESTS', 100)),
        'RATE_LIMIT_WINDOW': int(os.getenv('RATE_LIMIT_WINDOW', 60)),
        
        # Logging
        'LOG_LEVEL': os.getenv('LOG_LEVEL', 'INFO'),
    }
```

---

## 6. ‚öôÔ∏è Configuraci√≥n

### Archivo .env.example

```bash
# Servidor
HOST=0.0.0.0
PORT=8080
DEBUG=False

# Ollama (modelos locales)
OLLAMA_URL=http://localhost:11434

# DeepSeek API (fallback)
DEEPSEEK_URL=https://api.deepseek.com/chat/completions
DEEPSEEK_API_KEY=sk-tu-api-key-aqui

# API Keys (separadas por coma)
API_KEYS=sk-local-123,sk-production-456

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60

# Logging
LOG_LEVEL=INFO
```

---

## 7. üöÄ Deployment

### Opci√≥n 1: Docker

```bash
# docker-compose.yml
version: '3.8'

services:
  unified-ai-api:
    build: .
    ports:
      - "8080:8080"
    environment:
      - OLLAMA_URL=http://host.ollama:11434
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
    depends_on:
      - redis
    networks:
      - ai-network

  redis:
    image: redis:7-alpine
    networks:
      - ai-network

networks:
  ai-network:
    driver: bridge
```

### Opci√≥n 2: Systemd

```bash
# /etc/systemd/system/unified-ai-api.service
[Unit]
Description=Unified AI API Gateway
After=network.target ollama.service

[Service]
Type=simple
User=www-data
WorkingDirectory=/opt/unified-ai-api
Environment="PATH=/opt/unified-ai-api/venv/bin"
ExecStart=/opt/unified-ai-api/venv/bin/gunicorn \
    -w 4 \
    -k gevent \
    -b 0.0.0.0:8080 \
    run:app
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

### Opci√≥n 3: Nginx Reverse Proxy

```nginx
# /etc/nginx/sites-available/ai-api
server {
    listen 80;
    server_name ai-api.tudominio.com;
    
    location / {
        proxy_pass http://127.0.0.1:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Timeouts para requests largos
        proxy_connect_timeout 300s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }
}
```

---

## 8. üìñ Uso

### Ejemplo con cURL

```bash
# Usar modelo local
curl -X POST http://tu-servidor:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-tu-api-key" \
  -d '{
    "model": "qwen2.5:7b",
    "messages": [
      {"role": "system", "content": "Eres un asistente √∫til."},
      {"role": "user", "content": "Explicame qu√© es Python"}
    ],
    "temperature": 0.7
  }'
```

### Ejemplo con Python

```python
import openai

client = openai.OpenAI(
    base_url="http://tu-servidor:8080/v1",
    api_key="sk-tu-api-key"
)

response = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "system", "content": "Eres un asistente √∫til."},
        {"role": "user", "content": "Hola, ¬øc√≥mo est√°s?"}
    ]
)

print(response.choices[0].message.content)
```

### Comparaci√≥n de Costos

| Escenario | Solo Cloud | Con Proxy Local |
|-----------|------------|-----------------|
| 1000 requests simples | $10-20 USD | $0 USD |
| 1000 requests complejos | $10-20 USD | $10-20 USD |
| Tareas mixtas | $15-25 USD | $5-10 USD |

**Ahorro estimado:** 50-70% en tareas simples

---

## 9. üõ†Ô∏è Mantenimiento

### Comandos √∫tiles

```bash
# Ver logs
sudo journalctl -u unified-ai-api -f

# Reiniciar servicio
sudo systemctl restart unified-ai-api

# Verificar estado
sudo systemctl status unified-ai-api

# Actualizar modelo local
ollama pull qwen2.5:7b

# Ver modelos disponibles
ollama list

# Monitorear uso de recursos
htop
```

### Monitoreo

El sistema incluye endpoint de health:

```bash
curl http://tu-servidor:8080/health
```

Respuesta:
```json
{
  "status": "healthy",
  "ollama": "connected",
  "deepseek": "connected",
  "models": ["qwen2.5:7b", "llama3.2:3b"]
}
```

---

## üìä Resumen de Costos

### Inversi√≥n Inicial

| Item | Costo |
|------|-------|
| VPS con 12 n√∫cleos, 24GB RAM | $50-100/mes |
| Dominio | $10-15/a√±o |

### Ahorro Mensual

| Concepto | Antes | Despu√©s |
|----------|-------|---------|
| Tokens Cloud | $200-300 | $50-100 |
| **Ahorro** | - | **$150-200/mes** |

**ROI:** 2-3 meses

---

## üìß Contacto

**Desarrollado por:** IAFasioBoy (Asistente IA de Felix Rodr√≠guez Roble)  
**Email:** felix.roble@tecnotactil.com  
**Fecha:** 31 de enero de 2026

---

## ‚úÖ Pr√≥ximos Pasos

1. [ ] Obtener VPS con 12 n√∫cleos, 24GB RAM
2. [ ] Instalar Ollama y modelos
3. [ ] Desplegar Unified AI API
4. [ ] Configurar dominio y SSL
5. [ ] Migrar aplicaciones existentes

---

¬øDeseas que proceda con la implementaci√≥n completa o necesitas m√°s detalles en alguna secci√≥n espec√≠fica?
